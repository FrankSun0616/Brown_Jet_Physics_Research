# -*- coding: utf-8 -*-
"""Jet_DNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17M8fwqOjtTz05EUXx_kAU2NuCeA--Bjj
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

!pip install energyflow
import energyflow as ef

# Step 1: Load the Data
# Load SIM jets as an example; adjust 'dataset' and other parameters as necessary
modds = ef.mod.load(amount=1.0, dataset='sim', subdatasets={'SIM170_Jet300_pT375-infGeV'}, store_pfcs=True, store_gens=True, verbose=1)

# Step 2: Data Preprocessing
jet_etas = modds.jet_etas
gen_jet_etas = modds.gen_jet_etas  # GEN jet pts; target for regression

features_etas = jet_etas.reshape(-1, 1)#make sure the entry has shape (XXX,1)
targets_etas = gen_jet_etas  # True energy values

# Split into training and testing sets
etasx_train, etasx_test, etasy_train, etasy_test = train_test_split(features_etas, targets_etas, test_size=0.2, random_state=42)
print(gen_jet_etas)

# Step 3: Building the DNN Model
model_etas = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model_etas.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

#Step 4: Training and Evaluation
history = model_etas.fit(etasx_train, etasy_train, epochs=100, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)])

#Evaluate model performance
test_loss, test_mae = model_etas.evaluate(etasx_test, etasy_test)
print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')

predictions_etas = model_etas.predict(etasx_test).flatten()
plt.scatter(etasy_test, predictions_etas, alpha=0.5)
plt.xlabel('True GEN Jet etas')
plt.ylabel('Predicted GEN Jet etas')
plt.plot([min(etasy_test), max(etasy_test)], [min(etasy_test), max(etasy_test)], 'r--')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'predictions' and 'y_test' are already defined and contain your model's predictions and the actual values, respectively
# Ensure 'y_test' and 'predictions' are correctly shaped
predictions_etas = model_etas.predict(etasx_test).flatten()  # This line assumes 'model' and 'X_test' are already defined
y_test_flat = etasy_test.flatten()  # Ensuring y_test is a flat array for consistent operations
x_test_flat = etasx_test.flatten()
# Calculate the differences between predictions and true values
errors = predictions_etas - y_test_flat

# Define ranges
ranges = [(-2,2)]

# Loop through each range and create a histogram for each
for lower_bound, upper_bound in ranges:
    # Find indices where true values fall within the current range
    indices = (x_test_flat >= lower_bound) & (x_test_flat < upper_bound)

    # Filter errors based on indices
    filtered_errors = errors[indices]

    # Plot a histogram of the filtered errors
    plt.figure()  # Create a new figure for each histogram
    plt.hist(filtered_errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
    plt.xlabel('Prediction Error (Predicted - True)')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Prediction Errors for True Values in {lower_bound}-{upper_bound}')
    plt.grid(True)
    plt.show()

jet_pts = modds.jet_pts
jet_etas = modds.jet_etas
gen_jet_pts = modds.gen_jet_pts  # GEN jet pts; target for regression

# Stack features without normalization
# features = jet_pts.reshape(-1, 1)
# targets = gen_jet_pts  # True energy values
features = gen_jet_pts.reshape(-1, 1)
targets = jet_pts  # True energy values

# Split into training and testing sets without scaling features
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2)
#random_state=42

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.02, beta_1=0.9, beta_2=0.999), loss='MeanSquaredError', metrics=['mae'])
#loss function: 1.MeanSquaredError, 2.MeanAbsoluteError, 3.MeanSquaredLogarithmicError not useful, 4.MeanAbsolutePercentageError, 5.LogCosh, 6.tf.keras.losses.Huber(delta=1.0), 7.tf.keras.losses.CosineSimilarity(axis=1)not useful for this one
#optimizer: tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999), tf.keras.optimizers.Adamax(learning_rate=0.02, beta_1=0.9, beta_2=0.999), tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)not useful for this one

# Train the model
history = model.fit(X_train, y_train, epochs=100, validation_split=0.1)
#callbacks=[tf.keras.callbacks.EarlyStopping( verbose=1)]

# Evaluate model performance
test_loss, test_mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')

# Predictions
predictions = model.predict(X_test).flatten()
print(predictions)

# Scatter plot
plt.scatter(y_test, predictions, alpha=0.5)
plt.xlabel('True GEN Jet PT')
plt.ylabel('Predicted GEN Jet PT')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

predictions = model.predict(X_test).flatten()
y_test_flat = y_test.flatten()


errors = predictions - y_test_flat

# Define ranges
#ranges = [(300, 400), (400, 450), (450, 500)]
ranges = [(300, 500)]
for lower_bound, upper_bound in ranges:
    indices = (y_test_flat >= lower_bound) & (y_test_flat < upper_bound)

    filtered_errors = errors[indices]

    plt.figure()
    plt.hist(filtered_errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
    plt.xlabel('Prediction Error (Predicted - True)')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Prediction Errors for True Values in {lower_bound}-{upper_bound}')
    plt.grid(True)
    plt.show()

import numpy as np
import matplotlib.pyplot as plt

predictions = model.predict(X_test).flatten()
y_test_flat = y_test.flatten()

errors = predictions - y_test_flat

# Define ranges
ranges = [(300, 400), (400, 450), (450, 500)]
#ranges = [(300, 500)]
for lower_bound, upper_bound in ranges:
    indices = (y_test_flat >= lower_bound) & (y_test_flat < upper_bound)

    filtered_errors = errors[indices]

    plt.figure()
    plt.hist(filtered_errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
    plt.xlabel('Prediction Error (Predicted - True)')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Prediction Errors for True Values in {lower_bound}-{upper_bound}')
    plt.grid(True)
    plt.show()

import numpy as np
import matplotlib.pyplot as plt

predictions = model.predict(X_test).flatten()
y_test_flat = y_test.flatten()

# Calculate the differences between predictions and true values
errors = predictions - y_test_flat

ranges = [(300, 400), (400, 450), (450, 500)]
#ranges = [(300, 500)]

for lower_bound, upper_bound in ranges:
    indices = (X_test[:, 0] >= lower_bound) & (X_test[:, 0] < upper_bound)

    # Filter errors based on the indices determined by reconstructed jet pT
    filtered_errors = errors[indices]

    # Plot a histogram of the filtered errors
    plt.figure()
    plt.hist(filtered_errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
    plt.xlabel('Prediction Error (Predicted - True)')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Prediction Errors for Reconstructed Jet pT in {lower_bound}-{upper_bound}')
    plt.grid(True)
    plt.show()

# Predictions
predictions = model.predict(X_test).flatten()
# Scatter plot
plt.scatter(predictions, y_test, alpha=0.5)
plt.xlabel('predicted Reco Jet PT')
plt.ylabel('Reco Jet PT')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.show()
plt.scatter(X_test, y_test, alpha=0.5)
plt.xlabel('predicted Reco Jet PT')
plt.ylabel('Reco Jet PT')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.show()

synthetic_jet_pts = np.linspace(200, 600, 400).reshape(-1, 1)
synthetic_predictions = model.predict(synthetic_jet_pts).flatten()

plt.scatter(synthetic_jet_pts, synthetic_predictions, alpha=0.5, label='Predicted True GEN Jet PT (Synthetic)')

# Add reference line
plt.plot([200, 600], [200, 600], 'r--', label='Ideal Prediction Line')

plt.xlabel('Reco Jet PT')
plt.ylabel('Predicted True GEN Jet PT')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

synthetic_jet_pts = np.linspace(200, 600, 400).reshape(-1, 1)
synthetic_predictions = model.predict(synthetic_jet_pts).flatten()

plt.scatter(X_test[:, 0], y_test, alpha=0.5, label='True GEN Jet PT (Test Data)')

# Overlay with synthetic data predictions (Predicted GEN Jet PT vs. Synthetic Reco Jet PT)
plt.scatter(synthetic_jet_pts, synthetic_predictions, alpha=0.5, color='red', label='Predicted True GEN Jet PT (Synthetic)')

# Add reference line for synthetic predictions to ideal scenario
plt.plot([200, 600], [200, 600], 'g--', label='Ideal Prediction Line')


plt.xlabel('Reco Jet PT')
plt.ylabel('GEN Jet PT')
plt.legend()
plt.show()

#The jet PT response is defined as the ratio of the predicted GEN Jet PT to the true GEN Jet PT.
jet_pt_response = predictions / y_test
#Plot Jet response
plt.figure(figsize=(10, 6))
plt.hist(jet_pt_response, bins=50, range=(0.5, 1.5), alpha=0.7, label='Jet PT Response')
plt.xlabel('Predicted GEN Jet PT / True GEN Jet PT')
plt.ylabel('Number of Jets')
plt.title('Jet PT Response')
plt.legend()
plt.grid(True)
plt.show()

#Jet PT resolution: standard deviation of the jet PT response in bins of true GEN Jet PT.
bin_edges = np.linspace(200, 600, 21)#bin size changable
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
resolution = []

for i in range(len(bin_edges)-1):
    bin_mask = (y_test >= bin_edges[i]) & (y_test < bin_edges[i+1])
    responses_in_bin = jet_pt_response[bin_mask]
    resolution.append(np.std(responses_in_bin))

plt.figure(figsize=(10, 6))
plt.plot(bin_centers, resolution, marker='o', linestyle='-', color='blue', label='Jet PT Resolution')
plt.xlabel('True GEN Jet PT')
plt.ylabel('Resolution (STD of Response)')
plt.title('Jet PT Resolution')
plt.legend()
plt.grid(True)
plt.show()

bin_edges = np.arange(200, 650, 10)  # Creates bin edges from 200 to 600 in steps of 50
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculates the center of each bin
responses = []
response_errors = []

for i in range(len(bin_edges)-1):
    # Find indices of y_test that fall into the current bin
    indices = (y_test >= bin_edges[i]) & (y_test < bin_edges[i+1])

    # Calculate the response ratio for these indices
    response_ratios = predictions[indices] / y_test[indices]

    # Calculate mean response and standard deviation (error) for the current bin
    mean_response = np.mean(response_ratios)
    std_response = np.std(response_ratios)

    responses.append(mean_response)
    response_errors.append(std_response)

plt.figure(figsize=(8, 6))
plt.errorbar(bin_centers, responses, yerr=response_errors, fmt='o', label='Your Model', color='blue', ecolor='lightgray', elinewidth=3, capsize=0)
plt.plot([200, 600], [1, 1], 'r--', label='Ideal Response')  # Ideal response line at y=1
plt.xlabel(r'$p_T^{true} \ [GeV]$')
plt.ylabel(r'Jet $p_T$ Response ($p_T^{pred}/p_T^{true}$)')
plt.title('Jet $p_T$ Response')
plt.ylim(0.5, 1.5)  # Adjust as needed
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt


# Bin edges and centers
bin_edges = np.arange(200, 650, 10)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Initialize lists for responses and their standard deviation (as resolution)
responses = []
resolution = []
resolution_errors = []  # Assuming you have a measure of error for resolution

for i in range(len(bin_edges)-1):
    bin_mask = (y_test >= bin_edges[i]) & (y_test < bin_edges[i+1])
    response_ratios = predictions[bin_mask] / y_test[bin_mask]

    if len(response_ratios) > 0:
        responses.append(np.mean(response_ratios))
        resolution.append(np.std(response_ratios))
    else:
        responses.append(np.nan)
        resolution.append(np.nan)  # Placeholder for empty bins

# Assume resolution_errors as a simple example
resolution_errors = [0.01] * len(resolution)

# Plotting
plt.figure(figsize=(8, 6))
plt.plot(bin_centers, resolution, 'o-', label='Your Model', color='blue')
plt.fill_between(bin_centers, np.array(resolution) - np.array(resolution_errors),
                 np.array(resolution) + np.array(resolution_errors), color='blue', alpha=0.2)
plt.xlabel(r'$p_T^{true} \ [GeV]$')
plt.ylabel(r'Jet $p_T$ Resolution ($\sigma$)')
plt.title('Jet $p_T$ Resolution')
plt.legend()
plt.grid(True)
plt.show()

#High level variables into the training
jet_pts = modds.jet_pts
jet_etas = modds.jet_etas
gen_jet_pts = modds.gen_jet_pts
gen_jet_etas = modds.gen_jet_etas# as many as possible

features_variable = np.vstack((jet_pts, jet_etas)).T  # .T to transpose and match the required shape
#add any available above

targets_variable = gen_jet_pts
#targets_variable = np.vstack((gen_jet_pts, gen_jet_etas)).T
x_variable_train, x_variable_test, y_variable_train, y_variable_test = train_test_split(features_variable, targets_variable, test_size=0.2, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(features_variable.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(x_variable_train, y_variable_train, epochs=100, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping( verbose=1)])
test_loss = model.evaluate(x_variable_test, y_variable_test)
print(f'Test MSE Loss: {test_loss}')

predictions_variable = model.predict(x_variable_test).flatten()
import matplotlib.pyplot as plt
plt.scatter(y_variable_test, predictions_variable, alpha=0.5)
plt.xlabel('GEN Jet PT')
plt.ylabel('Predicted GEN Jet PT')
plt.plot([min(y_variable_test), max(y_variable_test)], [min(y_variable_test), max(y_variable_test)], 'r--')
plt.show()